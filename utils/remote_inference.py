from easyjailbreak.models.openai_model import OpenaiModel
from together import Together
from openai import OpenAI
from fastchat.conversation import get_conv_template, Conversation
import os
import json
import requests
from copy import deepcopy
from utils.constants import TOGETHER_API_KEY, OPENAI_API_KEY
import time


class RemoteInferenceModel(OpenaiModel):
    def __init__(
        self,
        model_name: str,
        api_key: str = None,
        generation_config=None,
        # host a local guard:
        # 1. we want the guard to truncate from the middle
        # 2. few API providers serve llama guard standalone
        # 3. we want to share the guard across all processes on this machine
        guard_url: str = None,
    ):
        """
        Initializes the OpenAI model with necessary parameters.
        :param str model_name: The name of the model to use.
        :param str api_key: API keys for accessing the OpenAI service.
        :param str template_name: The name of the conversation template, defaults to 'chatgpt'.
        :param dict generation_config: Configuration settings for generation, defaults to an empty dictionary.
        """
        if model_name.startswith("gpt"):
            if api_key is None:
                api_key = OPENAI_API_KEY
            self.client = OpenAI(api_key=api_key, max_retries=10)
        else:
            if api_key is None:
                api_key = TOGETHER_API_KEY
            self.client = Together(api_key=api_key, max_retries=25)
        self.api_key = api_key
        self.model_name = model_name
        self.generation_config = (
            generation_config if generation_config is not None else {}
        )
        self.conversation = get_conv_template("chatgpt")

        self.guard_url = guard_url
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)

    def set_system_message(self, system_message: str):
        """
        Sets a system message for the conversation.
        :param str system_message: The system message to set.
        """
        raise Exception("not thread safe")

    def set_system_message_thread_safe(self, system_message: str):
        res = RemoteInferenceModel(
            self.model_name, self.api_key, self.generation_config
        )
        res.conversation.system_message = system_message
        return res

    def blocked_by_guard(self, messages):
        if self.guard_url:
            print("posting", self.guard_url)
            max_retries = 5
            backoff_factor = 1
            for attempt in range(max_retries):
                try:
                    # Make the POST request
                    response = self.session.post(self.guard_url, json={"messages": messages})

                    # Check if the request was successful
                    if response.status_code == 200:
                        result = response.json()
                        return result["is_jailbreak"]
                    else:
                        raise Exception(f"Error calling guard {response}")
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    print(f"Attempt {attempt + 1} failed: {e}")
                    if attempt < max_retries - 1:
                        sleep_time = backoff_factor * (2 ** attempt)
                        print(f"Retrying in {sleep_time} seconds...")
                        time.sleep(sleep_time)
                    else:
                        return True
        return False

    def generate(self, messages, clear_old_history=True, **kwargs):
        """
        Generates a response based on messages that include conversation history.
        :param list[str]|str messages: A list of messages or a single message string.
                                       User and assistant messages should alternate.
        :param bool clear_old_history: If True, clears the old conversation history before adding new messages.
        :return str: The response generated by the OpenAI model based on the conversation history.
        """
        if not clear_old_history:
            raise Exception("not thread safe")

        conversation = get_conv_template("chatgpt")
        conversation.system_message = self.conversation.system_message
        if isinstance(messages, str) or isinstance(messages, dict):
            messages = [messages]

        assert isinstance(messages, list)

        if isinstance(messages, list) and all(isinstance(m, dict) for m in messages):
            # OpenAI message format
            for message in messages:
                if message["role"] == "system":
                    if (
                        conversation.system_message is not None
                        and message["content"] != conversation.system_message
                    ):
                        raise Exception("probable bug: system message changed")
                    conversation.system_message = message["content"]
                else:
                    conversation.append_message(message["role"], message["content"])
        else:
            for index, message in enumerate(messages):
                conversation.append_message(conversation.roles[index % 2], message)
        messages = conversation.to_openai_api_messages()

        if self.blocked_by_guard(messages):
            return "I'm sorry, I cannot fulfil that request"

        # print("doing inference")
        # for i in messages:
        #     print(f"{i['role']}: {i['content'][:100]}")
        # print("done inference")
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                **kwargs,
                **self.generation_config
            )
        except Exception as e:
            print(f"Inference: {e} with messages={messages}")
            raise
        return response.choices[0].message.content

    def batch_generate(self, conversations, **kwargs):
        """
        Generates responses for multiple conversations in a batch.
        :param list[list[str]]|list[str] conversations: A list of conversations, each as a list of messages.
        :return list[str]: A list of responses for each conversation.
        """
        responses = []
        for conversation in conversations:
            if isinstance(conversation, str):
                raise Exception("Must provide list[list[str]]")
            responses.append(self.generate(conversation, **kwargs))
        return responses


class Converser:
    def __init__(self, model, system_prompt=None, chat=None, **kwargs):
        self.inference = RemoteInferenceModel(model, **kwargs)
        self.model = model
        self.system_prompt = system_prompt
        self.kwargs = kwargs
        if system_prompt is not None:
            self.inference = self.inference.set_system_message_thread_safe(
                system_prompt
            )
        self.chat = chat or []

    def set_system_prompt(self, system_prompt):
        if self.model.startswith("claude"):
            self.system_prompt = system_prompt
        else:
            if self.chat and self.chat[0]["role"] == "system":
                self.chat.pop(0)
            self.chat.insert(0, {"role": "system", "content": system_prompt})

    def clone(self):
        return Converser(
            self.model,
            system_prompt=self.system_prompt,
            chat=deepcopy(self.chat),
            **self.kwargs,
        )

    def say(self, message, prefill=None, **kwargs):
        if message is not None:
            self.chat.append({"role": "user", "content": message})
        messages = self.chat
        for i in range(1, len(messages)):
            if messages[i]["role"] == messages[i - 1]["role"]:
                print(f"WARNING: DUPLICATE ROLES: {json.dumps(messages, indent=4)}")
                break
        if prefill:
            if self.model.startswith("gpt"):
                raise Exception("OpenAI does not support prefilling")
            messages = messages + [{"role": "assistant", "content": prefill}]
        response = self.inference.generate(messages=messages, **kwargs)
        # print(f"Converser attempting inference on {messages} -> {response}")
        if prefill:
            response = prefill + response
        self.chat.append({"role": "assistant", "content": response})
        return response

    def last(self, role="assistant"):
        for i in range(len(self.chat) - 1, -1, -1):
            chat = self.chat[i]
            if chat["role"] == role:
                return chat["content"]

    def fork(self, message, **kwargs):
        m = self.clone()
        m.say(message, **kwargs)
        return m

    def __str__(self):
        summary = f"Model: {self.model}\nChat History:\n"
        for message in self.chat:
            content = message["content"]
            if len(content) <= 100:
                summary += f"[{message['role']}] {content}\n"
            else:
                summary += f"[{message['role']}] {content[:50]}...{content[-50:]}\n"
        return summary.strip()
